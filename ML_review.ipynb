{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "try:\n",
    "    sc\n",
    "    spark\n",
    "except NameError:\n",
    "    sc = SparkContext('local')\n",
    "    spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+\n",
      "|           review_id|             user_id|         business_id|               stars|                date|                text|       useful|               funny|                cool|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+\n",
      "|I wouldn't come h...| can't speak for ...| most restaurants...|                   2|                   0|                   1|         null|                null|                null|\n",
      "|FzjInKYfM4K1Q5Hv9...|_ZAaHKOMDtvpWJvSp...|Huo1lJmVkdqvFuLtA...|                   5|          2013-05-13|Very friendly hom...|            1|                   0|                   1|\n",
      "|0hGNzXujl2rvlFZ36...|VhkjxxBz-A4M-mMnc...|9uj0o9kwe6FQNspkc...|                   1|          2016-05-21|Updating my revie...|         null|                null|                null|\n",
      "|HpXecdzMVspmCOkid...|hPDHB1Z6S1S-c9JgV...|ESinwiCgoJNG-tMWx...|                   5|          2014-05-25|My wife and I new...|            1|                   0|                   0|\n",
      "|The boyfriend and...| the ENTIRE resta...| gave us menus an...| only saying abou...| she quite litera...| never to be seen...| After eating| we sat there for...| she didn't come ...|\n",
      "|rIW89gPa5Ns4hIHhC...|G7awB625oNt_m_Dq_...|h5qBxa_L-pIdNSOBQ...|                   5|          2017-08-23|Not only is their...|            0|                   0|                   0|\n",
      "|There are interne...| I know that they...| though I do beli...|                   2|                   0|                   0|         null|                null|                null|\n",
      "|bKdPitLt6OFCHhRdj...|AyK9b__pYqdnt_Eqm...|-0b86isaXMY0v4g-V...|                   5|          2014-12-05|Love love love th...|            0|                   0|                   0|\n",
      "|K8HXgHmlvSpHTJ64u...|5ISQZfzgLzlyiMK-9...|uXoaycCmhc3Cg4Iei...|                   5|          2015-10-14|My husband and I ...|            0|                   0|                   0|\n",
      "|Scout is definite...|         accessories|            skincare|             jewelry|      laptop sleeves|            t-shirts|    keychains|             buttons| even BABY STUFF....|\n",
      "|QeSAfYfc5CUpoV8oD...|QB8c9Hyt6lL7ncIxz...|o__g2Q64FnNc_Q4O7...|                   4|          2016-06-12|Great selection o...|            1|                   1|                   1|\n",
      "|tXktsIIDBurS6xVmR...|ioKQ2j6gmVPktcCHz...|v0bL0-frMFrvHz-5O...|                   5|          2014-01-06|Awesome chicken. ...|            1|                   1|                   1|\n",
      "|MBjSKa0e3XE0wL7m6...|Iu5isy8VNf1d81o7z...|zpoZ6WyQUYff18-z4...|                   3|          2014-06-03|The Goodwich is a...|         null|                null|                null|\n",
      "|Rru9fLcfBFut-CkTp...|ZAfQEyVZ9yerd7lgR...|bvow46stEejfxnnqE...|                   3|          2016-05-30|This place is rea...|         null|                null|                null|\n",
      "|9e81Ct4d3d0Tep9kw...|y_rWaEMIio7RTa8kP...|Bb8Ui0qEU4ue888Ag...|                   4|          2016-06-20|Riesige Portionen...|            1|                   0|                   1|\n",
      "|FxscEHbd-csqiwTce...|EqP7a7yFgxyNPeA6B...|HNQzWfhR8kIgTsoxL...|                   4|          2015-07-16|Before I tell you...|         null|                null|                null|\n",
      "|fApTR_sbqHIdPNApG...|whkCSjDBf-QoVlnrF...|P2GBKrx7dJg3xeGtu...|                   4|          2011-02-21|I love the Bluewa...|            0|                   0|                   0|\n",
      "|u_2t034E58uBY4ovR...|IXZnAPyWLdHnwlRcf...|Ck4L85PeJF9UTtgg9...|                   5|          2017-11-02|Awesome doctor! R...|            0|                   0|                   0|\n",
      "|HhWovkYWA3Y58xJc0...|FVfGXRIQ6Trnx5cKa...|RESDUcs7fIiihp38-...|                   4|          2014-10-25|Friends who ate h...|         null|                null|                null|\n",
      "|Bzl5Fo4jCKDcWT1YQ...|fcfFXPoxpLHvR98cv...|A8MmwHrMI0WnRTyQM...|                   2|          2014-05-29|I was super excit...|         null|                null|                null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "try:\n",
    "    sc\n",
    "    spark\n",
    "except NameError:\n",
    "    sc = SparkContext('local')\n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "# url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\n",
    "# spark.sparkContext.addFile(url)\n",
    "stars = spark.read.csv(\"yelp_review.csv\", sep=\",\", header=True)\n",
    "stars = stars.na.drop(subset=[\"text\", \"stars\"])\n",
    "stars = stars.sample(False, 0.0001, 0)\n",
    "\n",
    "# Show DataFrame\n",
    "stars.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pyspark\n",
    "from pyspark.sql.functions import length\n",
    "# # Create a length column to be used as a future feature \n",
    "stars = stars.withColumn('length', length(stars['text']))\n",
    "# data_df.show()\n",
    "import pyspark.sql.functions as func\n",
    "# means = df.groupby(\"id\").agg(func.mean(\"col1\"))\n",
    "from pyspark.sql.functions import col, expr, when\n",
    "\n",
    "five_stars = stars.withColumn(\n",
    "    'class', when((col(\"stars\") == '5'), 'positive')\n",
    "    .otherwise(\"negative\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pyspark\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "# Create all the features to the data set\n",
    "pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"token_text\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pyspark\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pyspark\n",
    "# Create a and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_stars.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "# %pyspark\n",
    "# Fit and transform the pipeline\n",
    "print(five_stars.count())\n",
    "cleaner = data_prep_pipeline.fit(five_stars)\n",
    "#cleaned = cleaner.transform(five_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.pipeline.PipelineModel'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cleaner))\n",
    "print(type(five_stars))\n",
    "\n",
    "cleaned = cleaner.transform(five_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(262145,[236232,2...|\n",
      "|  1.0|(262145,[9639,158...|\n",
      "|  0.0|(262145,[37852,58...|\n",
      "|  1.0|(262145,[14,1846,...|\n",
      "|  0.0|(262145,[21872,25...|\n",
      "|  1.0|(262145,[15889,39...|\n",
      "|  0.0|(262145,[8227,262...|\n",
      "|  1.0|(262145,[7367,963...|\n",
      "|  1.0|(262145,[3067,963...|\n",
      "|  0.0|(262145,[115898,2...|\n",
      "|  0.0|(262145,[4200,547...|\n",
      "|  1.0|(262145,[15889,82...|\n",
      "|  0.0|(262145,[9616,963...|\n",
      "|  0.0|(262145,[14,1109,...|\n",
      "|  0.0|(262145,[8968,190...|\n",
      "|  0.0|(262145,[4200,961...|\n",
      "|  0.0|(262145,[5377,963...|\n",
      "|  1.0|(262145,[15889,29...|\n",
      "|  0.0|(262145,[991,2315...|\n",
      "|  0.0|(262145,[5497,963...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %pyspark\n",
    "# Show label and resulting features\n",
    "# cleaned.select(['label', 'features']).show()\n",
    "cleaned.select(['label', 'features']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pyspark\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Break data down into a training set and a testing set\n",
    "training, testing = cleaned.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes()\n",
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----+----------+--------------------+------+-----+----+------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|           review_id|             user_id|         business_id|stars|      date|                text|useful|funny|cool|length|   class|label|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+-----+----------+--------------------+------+-----+----+------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|0hGNzXujl2rvlFZ36...|VhkjxxBz-A4M-mMnc...|9uj0o9kwe6FQNspkc...|    1|2016-05-21|Updating my revie...|  null| null|null|    42|negative|  0.0|[updating, my, re...|[updating, review...|(262144,[37852,58...|(262144,[37852,58...|(262145,[37852,58...|[-352.46885449981...|[0.99987931456913...|       0.0|\n",
      "|9e81Ct4d3d0Tep9kw...|y_rWaEMIio7RTa8kP...|Bb8Ui0qEU4ue888Ag...|    4|2016-06-20|Riesige Portionen...|     1|    0|   1|   148|negative|  0.0|[riesige, portion...|[riesige, portion...|(262144,[8968,190...|(262144,[8968,190...|(262145,[8968,190...|[-1531.0291970857...|[1.0,3.1656946447...|       0.0|\n",
      "|HpXecdzMVspmCOkid...|hPDHB1Z6S1S-c9JgV...|ESinwiCgoJNG-tMWx...|    5|2014-05-25|My wife and I new...|     1|    0|   0|   970|positive|  1.0|[my, wife, and, i...|[wife, new, area,...|(262144,[14,1846,...|(262144,[14,1846,...|(262145,[14,1846,...|[-7141.0824784388...|[1.0,4.2842129994...|       0.0|\n",
      "|I wouldn't come h...| can't speak for ...| most restaurants...|    2|         0|                   1|  null| null|null|     1|negative|  0.0|                 [1]|                 [1]|(262144,[236232],...|(262144,[236232],...|(262145,[236232,2...|[-35.830056077536...|[0.99959317356570...|       0.0|\n",
      "|FKaPaV8zbKjqvEQF5...|VmAbEyrmP-bh5q_3t...|R_ZlcX46pPdjhjmfd...|    5|2014-08-05|As a vacationer, ...|     0|    0|   0|   452|positive|  1.0|[as, a, vacatione...|[vacationer,, one...|(262144,[1109,767...|(262144,[1109,767...|(262145,[1109,767...|[-3391.0928645331...|[1.0,3.2299241414...|       0.0|\n",
      "+--------------------+--------------------+--------------------+-----+----------+--------------------+------+-----+----+------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %pyspark\n",
    "# Tranform the model with the testing data\n",
    "test_results = predictor.transform(testing)\n",
    "# print(testing.columns)\n",
    "# print(test_results.columns)\n",
    "# test_results.toPandas().to_csv(\"test_results.csv\")\n",
    "test_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting reviews was: 0.508880\n"
     ]
    }
   ],
   "source": [
    "# %pyspark\n",
    "# Use the Class Evaluator for a cleaner description\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.11.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
